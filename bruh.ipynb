{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CommaAI calibration challenge\n",
    "\n",
    "The goal of this project is to predict the orientation of a camera from a video.\n",
    "We predict the pitch and yaw of the camera in radians at every frame of the footage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tdata\n",
    "import torchvision\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "HEIGHT = 874\n",
    "WIDTH = 1164\n",
    "CHUNK_LENGTH = 30\n",
    "np.random.seed(127)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_windows(idx: np.ndarray) -> list:\n",
    "    # converts a list of indices to a list of windows\n",
    "    # where a window is a tuple of (start, end)\n",
    "    # where start and end are the indices of the window\n",
    "    # e.g. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -> [(0, 10)]\n",
    "    # e.g. [0, 1, 2, 3, 6, 7, 8, 9, 10] -> [(0, 4), (6, 11)]\n",
    "    # e.g. [0, 1, 9, 10, 11, 12, 13, 17, 18, 19] -> [(0, 2), (9, 14), (17, 20)]\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    jumps = np.argwhere(np.diff(idx) > 1).ravel()\n",
    "    if len(jumps) == 0:\n",
    "        return [(idx[0], idx[-1] + 1)]\n",
    "    else:\n",
    "        windows = []\n",
    "        for i in range(len(jumps)):\n",
    "            if i == 0:\n",
    "                windows.append((idx[0], idx[jumps[i]] + 1))\n",
    "            else:\n",
    "                windows.append((idx[jumps[i - 1] + 1], idx[jumps[i]] + 1))\n",
    "\n",
    "        windows.append((idx[jumps[-1] + 1], idx[-1] + 1))\n",
    "        return windows\n",
    "\n",
    "\n",
    "def nan_windows(arr: np.ndarray) -> list:\n",
    "    if np.all(~np.isnan(arr[:, 0])):\n",
    "        return []\n",
    "    else:\n",
    "        nan_indices = np.argwhere(np.isnan(arr[:, 0])).ravel()\n",
    "        return indices_to_windows(nan_indices)\n",
    "\n",
    "\n",
    "def non_nan_windows(arr: np.ndarray) -> list:\n",
    "    if np.all(np.isnan(arr[:, 0])):\n",
    "        return []\n",
    "    else:\n",
    "        non_nan_indices = np.argwhere(~np.isnan(arr[:, 0])).ravel()\n",
    "        return indices_to_windows(non_nan_indices)\n",
    "\n",
    "\n",
    "# print(indices_to_windows([0, 1, 2, 3, 6, 7, 8, 9, 10]))\n",
    "# print(indices_to_windows([0, 1, 9, 10, 11, 12, 13, 17, 18, 19]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = []\n",
    "orientations = []\n",
    "for i in range(5):\n",
    "    vframes, _, _ = torchvision.io.read_video(\n",
    "        f\"labeled/{i}.mp4\", start_pts=0, end_pts=60, pts_unit=\"sec\"\n",
    "    )\n",
    "    videos.append(vframes)\n",
    "    orientations.append(np.loadtxt(f\"labeled/{i}.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(videos)):\n",
    "    print(videos[i].shape, orientations[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each mp4 video in labeled/ and split it in the max number of contiguous 30 frames chunks\n",
    "# with non-nan orientation data\n",
    "# save each chunk as a separate mp4 file as labeled_chunks/{index}_{chunk_index}.mp4\n",
    "# save the orientation data for each chunk as labeled_chunks/{index}_{chunk_index}.txt\n",
    "# where index is the index of the original video and chunk_index is the index of the chunk\n",
    "# in the original video\n",
    "if not os.path.exists(\"labeled_chunks\"):\n",
    "    os.mkdir(\"labeled_chunks\")\n",
    "for video_id in range(5):\n",
    "    print(f\"processing video {video_id}\")\n",
    "    vframes = videos[video_id]\n",
    "    labels = orientations[video_id]\n",
    "    windows = non_nan_windows(labels)\n",
    "    chunk_id = 0\n",
    "    for window in windows:\n",
    "        start, end = window\n",
    "        print(f\"processing window {start},{end}\")\n",
    "        if end - start < CHUNK_LENGTH:\n",
    "            print(f\"skipping window {start},{end} because it's too short\")\n",
    "            continue\n",
    "        else:\n",
    "            num_chunks = (end - start) // CHUNK_LENGTH\n",
    "            for i in range(num_chunks):\n",
    "                chunk_start = start + i * CHUNK_LENGTH\n",
    "                chunk_end = chunk_start + CHUNK_LENGTH\n",
    "                chunk = vframes[chunk_start:chunk_end]\n",
    "                chunk_orientations = labels[chunk_start:chunk_end]\n",
    "                if np.any(np.isnan(chunk_orientations)):\n",
    "                    raise ValueError(\"chunk has nan orientation data\")\n",
    "\n",
    "                torchvision.io.write_video(\n",
    "                    f\"labeled_chunks/{video_id}_{chunk_id}.mp4\",\n",
    "                    chunk,\n",
    "                    fps=20,\n",
    "                    video_codec=\"libx264\",\n",
    "                )\n",
    "                np.savetxt(\n",
    "                    f\"labeled_chunks/{video_id}_{chunk_id}.txt\", chunk_orientations\n",
    "                )\n",
    "                print(\n",
    "                    f\"saved chunk {video_id}_{chunk_id} on window {chunk_start},{chunk_end}\"\n",
    "                )\n",
    "                chunk_id += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(tdata.Dataset):\n",
    "    data_dir: str\n",
    "    batch_size: int\n",
    "    filenames: list\n",
    "    data: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self, data_dir=\"labeled_chunks\", filenames: list = [], device=DEVICE\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.filenames = filenames\n",
    "\n",
    "        self.data = torch.zeros(\n",
    "            (len(self.filenames), CHUNK_LENGTH, HEIGHT, WIDTH, 3), device=device\n",
    "        )\n",
    "        self.labels = torch.zeros((len(self.filenames), CHUNK_LENGTH, 2), device=device)\n",
    "        for i, filename in enumerate(self.filenames):\n",
    "            frames, _, _ = torchvision.io.read_video(\n",
    "                os.path.join(self.data_dir, filename),\n",
    "                start_pts=0,\n",
    "                end_pts=CHUNK_LENGTH,\n",
    "                pts_unit=\"pts\",\n",
    "            )\n",
    "            self.data[i] = frames.to(device=device)\n",
    "            self.labels[i] = torch.from_numpy(\n",
    "                np.loadtxt(\n",
    "                    os.path.join(self.data_dir, filename.replace(\".mp4\", \".txt\"))\n",
    "                )\n",
    "            ).to(device=device)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple:\n",
    "        return self.data[index], self.labels[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir=\"labeled_chunks\", batch_size: int = 3) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        filenames = []\n",
    "        for filename in os.listdir(self.data_dir):\n",
    "            if len(filenames) >= 10:\n",
    "                break\n",
    "            if filename.endswith(\".mp4\"):\n",
    "                filenames.append(filename)\n",
    "        if len(filenames) == 0:\n",
    "            raise ValueError(\"no files\")\n",
    "        print(filenames)\n",
    "        np.random.shuffle(filenames)\n",
    "        num_train = int(len(filenames) * 0.8)\n",
    "        num_val = int(len(filenames) * 0.1)\n",
    "        num_test = len(filenames) - num_train - num_val\n",
    "        self.train_filenames = filenames[:num_train]\n",
    "        self.val_filenames = filenames[num_train : num_train + num_val]\n",
    "        self.test_filenames = filenames[num_train + num_val :]\n",
    "        self.train_data = torch.zeros(\n",
    "            (len(self.train_filenames), CHUNK_LENGTH, HEIGHT, WIDTH, 3)\n",
    "        )\n",
    "        self.val_data = torch.zeros(\n",
    "            (len(self.val_filenames), CHUNK_LENGTH, HEIGHT, WIDTH, 3)\n",
    "        )\n",
    "        self.test_data = torch.zeros(\n",
    "            (len(self.test_filenames), CHUNK_LENGTH, HEIGHT, WIDTH, 3)\n",
    "        )\n",
    "        self.train_labels = torch.zeros((len(self.train_filenames), CHUNK_LENGTH, 2))\n",
    "        self.val_labels = torch.zeros((len(self.val_filenames), CHUNK_LENGTH, 2))\n",
    "        self.test_labels = torch.zeros((len(self.test_filenames), CHUNK_LENGTH, 2))\n",
    "        for i, filename in enumerate(self.train_filenames):\n",
    "            frames, _, _ = torchvision.io.read_video(\n",
    "                os.path.join(self.data_dir, filename),\n",
    "                start_pts=0,\n",
    "                end_pts=CHUNK_LENGTH,\n",
    "                pts_unit=\"pts\",\n",
    "            )\n",
    "            self.train_data[i] = frames\n",
    "            self.train_labels[i] = torch.from_numpy(\n",
    "                np.loadtxt(\n",
    "                    os.path.join(self.data_dir, filename.replace(\".mp4\", \".txt\"))\n",
    "                )\n",
    "            )\n",
    "        for i, filename in enumerate(self.val_filenames):\n",
    "            frames, _, _ = torchvision.io.read_video(\n",
    "                os.path.join(self.data_dir, filename),\n",
    "                start_pts=0,\n",
    "                end_pts=CHUNK_LENGTH,\n",
    "                pts_unit=\"pts\",\n",
    "            )\n",
    "            self.val_data[i] = frames\n",
    "            self.val_labels[i] = torch.from_numpy(\n",
    "                np.loadtxt(\n",
    "                    os.path.join(self.data_dir, filename.replace(\".mp4\", \".txt\"))\n",
    "                )\n",
    "            )\n",
    "        for i, filename in enumerate(self.test_filenames):\n",
    "            frames, _, _ = torchvision.io.read_video(\n",
    "                os.path.join(self.data_dir, filename),\n",
    "                start_pts=0,\n",
    "                end_pts=CHUNK_LENGTH,\n",
    "                pts_unit=\"pts\",\n",
    "            )\n",
    "            self.test_data[i] = frames\n",
    "            self.test_labels[i] = torch.from_numpy(\n",
    "                np.loadtxt(\n",
    "                    os.path.join(self.data_dir, filename.replace(\".mp4\", \".txt\"))\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> tdata.DataLoader:\n",
    "        return tdata.DataLoader(\n",
    "            tdata.TensorDataset(self.train_data, self.train_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> tdata.DataLoader:\n",
    "        return tdata.DataLoader(\n",
    "            tdata.TensorDataset(self.val_data, self.val_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> tdata.DataLoader:\n",
    "        return tdata.DataLoader(\n",
    "            tdata.TensorDataset(self.test_data, self.test_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original ConvLSTM cell as proposed by Shi et al.\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "    ):\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        if activation == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = torch.relu\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels + out_channels,\n",
    "            out_channels=4 * out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # Initialize weights for Hadamard Products\n",
    "        self.W_ci = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "        self.W_co = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "        self.W_cf = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "\n",
    "    def forward(self, X, H_prev, C_prev):\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n",
    "\n",
    "        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev)\n",
    "        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev)\n",
    "\n",
    "        # Current Cell output\n",
    "        C = forget_gate * C_prev + input_gate * self.activation(C_conv)\n",
    "\n",
    "        output_gate = torch.sigmoid(o_conv + self.W_co * C)\n",
    "\n",
    "        # Current Hidden State\n",
    "        H = output_gate * self.activation(C)\n",
    "\n",
    "        return H, C\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "    ):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # We will unroll this over time steps\n",
    "        self.convLSTMcell = ConvLSTMCell(\n",
    "            in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is a frame sequence (batch_size, num_channels, seq_len, height, width)\n",
    "\n",
    "        # Get the dimensions\n",
    "        batch_size, _, seq_len, height, width = X.size()\n",
    "\n",
    "        # Initialize output\n",
    "        output = torch.zeros(\n",
    "            batch_size, self.out_channels, seq_len, height, width, device=DEVICE\n",
    "        )\n",
    "\n",
    "        # Initialize Hidden State\n",
    "        H = torch.zeros(batch_size, self.out_channels, height, width, device=DEVICE)\n",
    "\n",
    "        # Initialize Cell Input\n",
    "        C = torch.zeros(batch_size, self.out_channels, height, width, device=DEVICE)\n",
    "\n",
    "        # Unroll over time steps\n",
    "        for time_step in range(seq_len):\n",
    "            H, C = self.convLSTMcell(X[:, :, time_step], H, C)\n",
    "\n",
    "            output[:, :, time_step] = H\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OrientationEstimator(nn.Module):\n",
    "    sequential: nn.Sequential\n",
    "    conv: nn.Conv2d\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_kernels: int = 64,\n",
    "        kernel_size: tuple = (3, 3),\n",
    "        padding: tuple = (1, 1),\n",
    "        activation: str = \"relu\",\n",
    "        frame_size: tuple = (HEIGHT, WIDTH),\n",
    "        num_layers: int = 1,\n",
    "    ):\n",
    "        super(OrientationEstimator, self).__init__()\n",
    "\n",
    "        self.sequential = nn.Sequential()\n",
    "\n",
    "        # Add First layer (Different in_channels than the rest)\n",
    "        self.sequential.add_module(\n",
    "            \"convlstm1\",\n",
    "            ConvLSTM(\n",
    "                in_channels=3,\n",
    "                out_channels=num_kernels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                activation=activation,\n",
    "                frame_size=frame_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.sequential.add_module(\n",
    "            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "        )\n",
    "\n",
    "        # Add rest of the layers\n",
    "        for l in range(2, num_layers + 1):\n",
    "            self.sequential.add_module(\n",
    "                f\"convlstm{l}\",\n",
    "                ConvLSTM(\n",
    "                    in_channels=num_kernels,\n",
    "                    out_channels=num_kernels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    activation=activation,\n",
    "                    frame_size=frame_size,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            self.sequential.add_module(\n",
    "                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "            )\n",
    "\n",
    "        # Add final Convolutional Layer to predict camera orientation\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=num_kernels,\n",
    "            out_channels=2,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation through all the layers\n",
    "        output = self.sequential(X)\n",
    "\n",
    "        # Return only the last output frame\n",
    "        print(\"pre-conv, output.shape=\", output.shape)\n",
    "        return self.conv(output[:, :, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels: int = 3,\n",
    "        num_kernels: int = 64,\n",
    "        kernel_size: tuple = (3, 3),\n",
    "        padding: tuple = (1, 1),\n",
    "        activation: str = \"relu\",\n",
    "        frame_size: tuple = (HEIGHT, WIDTH),\n",
    "        num_layers: int = 1,\n",
    "    ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.sequential = nn.Sequential()\n",
    "\n",
    "        # Add First layer (Different in_channels than the rest)\n",
    "        self.sequential.add_module(\n",
    "            \"convlstm1\",\n",
    "            ConvLSTM(\n",
    "                in_channels=num_channels,\n",
    "                out_channels=num_kernels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                activation=activation,\n",
    "                frame_size=frame_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.sequential.add_module(\n",
    "            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "        )\n",
    "\n",
    "        # Add rest of the layers\n",
    "        for l in range(2, num_layers + 1):\n",
    "            self.sequential.add_module(\n",
    "                f\"convlstm{l}\",\n",
    "                ConvLSTM(\n",
    "                    in_channels=num_kernels,\n",
    "                    out_channels=num_kernels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    activation=activation,\n",
    "                    frame_size=frame_size,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            self.sequential.add_module(\n",
    "                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "            )\n",
    "\n",
    "            # Add Convolutional Layer to predict output frame\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=num_kernels,\n",
    "            out_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation through all the layers\n",
    "        output = self.sequential(X)\n",
    "\n",
    "        # Return only the last output frame\n",
    "        output = self.conv(output[:, :, -1])\n",
    "\n",
    "        return nn.Sigmoid()(output)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=2,\n",
    ")\n",
    "model = Seq2Seq()\n",
    "data_module = DataModule(\n",
    "    data_dir=DATA_DIR\n",
    ")\n",
    "trainer.fit(model, data_module)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: OrientationEstimator,\n",
    "    loader: tdata.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "):\n",
    "    model.train()\n",
    "    for batch_idx, (X, y) in enumerate(loader):\n",
    "        X = X.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        y_hat = model(X)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sys.stdout.write(f\"Batch {batch_idx}/{len(loader)} Loss: {loss.item()}\\r\")\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def get_mse(gt, test):\n",
    "    test = np.nan_to_num(test)\n",
    "    return np.mean(np.nanmean((gt - test) ** 2, axis=0))\n",
    "\n",
    "\n",
    "def eval_epoch(model: OrientationEstimator, loader: tdata.DataLoader):\n",
    "    model.eval()\n",
    "    zero_mses = []\n",
    "    mses = []\n",
    "    loss_mses = []\n",
    "    for batch_idx, (X, y) in enumerate(loader):\n",
    "        X = X.to(DEVICE)\n",
    "        y_hat = model(X)\n",
    "        y = y.to(DEVICE)\n",
    "        loss_mses.append(F.mse_loss(y_hat, y).item())\n",
    "        y = y.cpu().detach().numpy()\n",
    "        y_hat = y_hat.cpu().detach().numpy()\n",
    "        zero_mses.append(get_mse(y, np.zeros_like(y)))\n",
    "        mses.append(get_mse(y, y_hat))\n",
    "    return loss_mses, 100 * np.mean(mses) / np.mean(zero_mses)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model_name: str,\n",
    "    model: OrientationEstimator,\n",
    "    train_loader: tdata.DataLoader,\n",
    "    val_loader: tdata.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_epochs: int = 1,\n",
    "):\n",
    "    best_val_err = 100\n",
    "    with open(f\"{model_name}.log\", \"w\") as f:\n",
    "        f.write(\"epoch,train_loss,val_loss,val_err\")\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = train_epoch(model, train_loader, optimizer)\n",
    "            val_loss, val_err = eval_epoch(model, val_loader)\n",
    "            print(\n",
    "                f\"\\nEpoch {epoch}/{num_epochs} Train Loss: {train_loss}, Val loss: {val_loss}, Val err: {val_err}\"\n",
    "            )\n",
    "            f.write(f\"{epoch},{train_loss},{val_loss},{val_err}\")\n",
    "            if val_err < best_val_err:\n",
    "                best_val_err = val_err\n",
    "                torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
    "                print(\"Saved model at epoch {} with val err {}\".format(epoch, val_err))\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"/kaggle/input/commaai-calibration-challenge-dataset/labeled_chunks\"\n",
    "# get filenames of all mp4 files\n",
    "filenames = glob.glob(os.path.join(prefix, \"**\", \"*.mp4\"), recursive=True)\n",
    "assert len(filenames) > 0\n",
    "# shuffle filenames and split them into train and validation\n",
    "indices = np.arange(len(filenames))\n",
    "np.random.shuffle(indices)\n",
    "# train_indices = indices[: int(0.8 * len(indices))]\n",
    "# val_indices = indices[int(0.8 * len(indices)) :]\n",
    "train_indices = indices[:20]\n",
    "val_indices = indices[20:25]\n",
    "train_filenames = [filenames[i] for i in train_indices]\n",
    "val_filenames = [filenames[i] for i in val_indices]\n",
    "# create train and validation datasets\n",
    "train_dataset = VideoDataset(\n",
    "    data_dir=prefix,\n",
    "    filenames=train_filenames,\n",
    "    device=\"cuda:1\"\n",
    ")\n",
    "val_dataset = VideoDataset(\n",
    "    data_dir=prefix,\n",
    "    filenames=val_filenames,\n",
    "    device=\"cuda:1\"\n",
    ")\n",
    "# create train and validation dataloaders\n",
    "train_loader = tdata.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = tdata.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_size(tensor):\n",
    "    return tensor.element_size() * tensor.nelement() / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and optimizer\n",
    "model = OrientationEstimator()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# train model\n",
    "train(\n",
    "    \"orientation_estimator\",\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    num_epochs=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find memory taken by a tensor in GB\n",
    "def get_tensor_size(tensor):\n",
    "    return tensor.element_size() * tensor.nelement() / 1e9\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fee673dc4fc01db422c313512934603878a992984f49c83d6d8be337f7be7ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
