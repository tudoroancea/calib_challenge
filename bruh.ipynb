{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CommaAI calibration challenge\n",
    "\n",
    "The goal of this project is to predict the orientation of a camera from a video.\n",
    "We predict the pitch and yaw of the camera in radians at every frame of the footage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tdata\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "HEIGHT = 874\n",
    "WIDTH = 1164\n",
    "MIN_CHUNK_LENGTH = 30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 874, 1164, 3])\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "vframes, _, _ = torchvision.io.read_video(\n",
    "    f\"labeled/{index}.mp4\", start_pts=0, end_pts=60, pts_unit=\"sec\"\n",
    ")\n",
    "print(vframes.shape)\n",
    "orientations = np.loadtxt(f\"labeled/{index}.txt\")\n",
    "print(orientations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4), (6, 11)]\n",
      "[(0, 2), (9, 14), (17, 20)]\n"
     ]
    }
   ],
   "source": [
    "def indices_to_windows(idx: np.ndarray)->list:\n",
    "    # converts a list of indices to a list of windows\n",
    "    # where a window is a tuple of (start, end)\n",
    "    # where start and end are the indices of the window\n",
    "    # e.g. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -> [(0, 10)]\n",
    "    # e.g. [0, 1, 2, 3, 6, 7, 8, 9, 10] -> [(0, 4), (6, 11)]\n",
    "    # e.g. [0, 1, 9, 10, 11, 12, 13, 17, 18, 19] -> [(0, 2), (9, 14), (17, 20)]\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    jumps = np.argwhere(np.diff(idx) > 1).ravel()\n",
    "    if len(jumps) == 0:\n",
    "        return [(idx[0], idx[-1] + 1)]\n",
    "    else:\n",
    "        windows = []\n",
    "        for i in range(len(jumps)):\n",
    "            if i == 0:\n",
    "                windows.append((idx[0], idx[jumps[i]] + 1))\n",
    "            else:\n",
    "                windows.append((idx[jumps[i - 1] + 1], idx[jumps[i]] + 1))\n",
    "\n",
    "        windows.append((idx[jumps[-1] + 1], idx[-1] + 1))\n",
    "        return windows\n",
    "    \n",
    "\n",
    "def nan_windows(arr: np.ndarray)->list:\n",
    "    if np.all(~np.isnan(arr[:, 0])):\n",
    "        return []\n",
    "    else:\n",
    "        nan_indices = np.argwhere(np.isnan(arr[:, 0])).ravel()\n",
    "        return indices_to_windows(nan_indices)\n",
    "\n",
    "def non_nan_windows(arr: np.ndarray)->list:\n",
    "    if np.all(np.isnan(arr[:, 0])):\n",
    "        return []\n",
    "    else:\n",
    "        non_nan_indices = np.argwhere(~np.isnan(arr[:, 0])).ravel()\n",
    "        return indices_to_windows(non_nan_indices)\n",
    "\n",
    "# print(indices_to_windows([0, 1, 2, 3, 6, 7, 8, 9, 10]))\n",
    "# print(indices_to_windows([0, 1, 9, 10, 11, 12, 13, 17, 18, 19]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = []\n",
    "orientations = []\n",
    "for i in range(5):\n",
    "    vframes, _, _ = torchvision.io.read_video(\n",
    "        f\"labeled/{i}.mp4\", start_pts=0, end_pts=60, pts_unit=\"sec\"\n",
    "    )\n",
    "    videos.append(vframes)\n",
    "    orientations.append(np.loadtxt(f\"labeled/{i}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 874, 1164, 3]) (1200, 2)\n",
      "torch.Size([1200, 874, 1164, 3]) (1200, 2)\n",
      "torch.Size([1200, 874, 1164, 3]) (1200, 2)\n",
      "torch.Size([1200, 874, 1164, 3]) (1200, 2)\n",
      "torch.Size([1196, 874, 1164, 3]) (1196, 2)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(videos)):\n",
    "    print(videos[i].shape, orientations[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing video 0\n",
      "processing window 0,1200\n",
      "saved chunk 0_0 on window 0,30\n",
      "saved chunk 0_1 on window 30,60\n",
      "saved chunk 0_2 on window 60,90\n",
      "saved chunk 0_3 on window 90,120\n",
      "saved chunk 0_4 on window 120,150\n",
      "saved chunk 0_5 on window 150,180\n",
      "saved chunk 0_6 on window 180,210\n",
      "saved chunk 0_7 on window 210,240\n",
      "saved chunk 0_8 on window 240,270\n",
      "saved chunk 0_9 on window 270,300\n",
      "saved chunk 0_10 on window 300,330\n",
      "saved chunk 0_11 on window 330,360\n",
      "saved chunk 0_12 on window 360,390\n",
      "saved chunk 0_13 on window 390,420\n",
      "saved chunk 0_14 on window 420,450\n",
      "saved chunk 0_15 on window 450,480\n",
      "saved chunk 0_16 on window 480,510\n",
      "saved chunk 0_17 on window 510,540\n",
      "saved chunk 0_18 on window 540,570\n",
      "saved chunk 0_19 on window 570,600\n",
      "saved chunk 0_20 on window 600,630\n",
      "saved chunk 0_21 on window 630,660\n",
      "saved chunk 0_22 on window 660,690\n",
      "saved chunk 0_23 on window 690,720\n",
      "saved chunk 0_24 on window 720,750\n",
      "saved chunk 0_25 on window 750,780\n",
      "saved chunk 0_26 on window 780,810\n",
      "saved chunk 0_27 on window 810,840\n",
      "saved chunk 0_28 on window 840,870\n",
      "saved chunk 0_29 on window 870,900\n",
      "saved chunk 0_30 on window 900,930\n",
      "saved chunk 0_31 on window 930,960\n",
      "saved chunk 0_32 on window 960,990\n",
      "saved chunk 0_33 on window 990,1020\n",
      "saved chunk 0_34 on window 1020,1050\n",
      "saved chunk 0_35 on window 1050,1080\n",
      "saved chunk 0_36 on window 1080,1110\n",
      "saved chunk 0_37 on window 1110,1140\n",
      "saved chunk 0_38 on window 1140,1170\n",
      "saved chunk 0_39 on window 1170,1200\n",
      "processing video 1\n",
      "processing window 0,1200\n",
      "saved chunk 1_0 on window 0,30\n",
      "saved chunk 1_1 on window 30,60\n",
      "saved chunk 1_2 on window 60,90\n",
      "saved chunk 1_3 on window 90,120\n",
      "saved chunk 1_4 on window 120,150\n",
      "saved chunk 1_5 on window 150,180\n",
      "saved chunk 1_6 on window 180,210\n",
      "saved chunk 1_7 on window 210,240\n",
      "saved chunk 1_8 on window 240,270\n",
      "saved chunk 1_9 on window 270,300\n",
      "saved chunk 1_10 on window 300,330\n",
      "saved chunk 1_11 on window 330,360\n",
      "saved chunk 1_12 on window 360,390\n",
      "saved chunk 1_13 on window 390,420\n",
      "saved chunk 1_14 on window 420,450\n",
      "saved chunk 1_15 on window 450,480\n",
      "saved chunk 1_16 on window 480,510\n",
      "saved chunk 1_17 on window 510,540\n",
      "saved chunk 1_18 on window 540,570\n",
      "saved chunk 1_19 on window 570,600\n",
      "saved chunk 1_20 on window 600,630\n",
      "saved chunk 1_21 on window 630,660\n",
      "saved chunk 1_22 on window 660,690\n",
      "saved chunk 1_23 on window 690,720\n",
      "saved chunk 1_24 on window 720,750\n",
      "saved chunk 1_25 on window 750,780\n",
      "saved chunk 1_26 on window 780,810\n",
      "saved chunk 1_27 on window 810,840\n",
      "saved chunk 1_28 on window 840,870\n",
      "saved chunk 1_29 on window 870,900\n",
      "saved chunk 1_30 on window 900,930\n",
      "saved chunk 1_31 on window 930,960\n",
      "saved chunk 1_32 on window 960,990\n",
      "saved chunk 1_33 on window 990,1020\n",
      "saved chunk 1_34 on window 1020,1050\n",
      "saved chunk 1_35 on window 1050,1080\n",
      "saved chunk 1_36 on window 1080,1110\n",
      "saved chunk 1_37 on window 1110,1140\n",
      "saved chunk 1_38 on window 1140,1170\n",
      "saved chunk 1_39 on window 1170,1200\n",
      "processing video 2\n",
      "processing window 0,343\n",
      "saved chunk 2_0 on window 0,30\n",
      "saved chunk 2_1 on window 30,60\n",
      "saved chunk 2_2 on window 60,90\n",
      "saved chunk 2_3 on window 90,120\n",
      "saved chunk 2_4 on window 120,150\n",
      "saved chunk 2_5 on window 150,180\n",
      "saved chunk 2_6 on window 180,210\n",
      "saved chunk 2_7 on window 210,240\n",
      "saved chunk 2_8 on window 240,270\n",
      "saved chunk 2_9 on window 270,300\n",
      "saved chunk 2_10 on window 300,330\n",
      "processing window 346,359\n",
      "skipping window 346,359 because it's too short\n",
      "processing window 360,362\n",
      "skipping window 360,362 because it's too short\n",
      "processing window 364,372\n",
      "skipping window 364,372 because it's too short\n",
      "processing window 373,377\n",
      "skipping window 373,377 because it's too short\n",
      "processing window 378,382\n",
      "skipping window 378,382 because it's too short\n",
      "processing window 383,391\n",
      "skipping window 383,391 because it's too short\n",
      "processing window 392,394\n",
      "skipping window 392,394 because it's too short\n",
      "processing window 395,398\n",
      "skipping window 395,398 because it's too short\n",
      "processing window 399,401\n",
      "skipping window 399,401 because it's too short\n",
      "processing window 402,403\n",
      "skipping window 402,403 because it's too short\n",
      "processing window 405,406\n",
      "skipping window 405,406 because it's too short\n",
      "processing window 408,409\n",
      "skipping window 408,409 because it's too short\n",
      "processing window 410,412\n",
      "skipping window 410,412 because it's too short\n",
      "processing window 413,414\n",
      "skipping window 413,414 because it's too short\n",
      "processing window 415,416\n",
      "skipping window 415,416 because it's too short\n",
      "processing window 417,418\n",
      "skipping window 417,418 because it's too short\n",
      "processing window 421,422\n",
      "skipping window 421,422 because it's too short\n",
      "processing window 423,424\n",
      "skipping window 423,424 because it's too short\n",
      "processing window 425,426\n",
      "skipping window 425,426 because it's too short\n",
      "processing window 429,430\n",
      "skipping window 429,430 because it's too short\n",
      "processing window 432,433\n",
      "skipping window 432,433 because it's too short\n",
      "processing window 439,440\n",
      "skipping window 439,440 because it's too short\n",
      "processing window 561,562\n",
      "skipping window 561,562 because it's too short\n",
      "processing window 938,939\n",
      "skipping window 938,939 because it's too short\n",
      "processing window 940,942\n",
      "skipping window 940,942 because it's too short\n",
      "processing window 943,949\n",
      "skipping window 943,949 because it's too short\n",
      "processing window 950,951\n",
      "skipping window 950,951 because it's too short\n",
      "processing window 953,956\n",
      "skipping window 953,956 because it's too short\n",
      "processing window 960,974\n",
      "skipping window 960,974 because it's too short\n",
      "processing window 975,1200\n",
      "saved chunk 2_11 on window 975,1005\n",
      "saved chunk 2_12 on window 1005,1035\n",
      "saved chunk 2_13 on window 1035,1065\n",
      "saved chunk 2_14 on window 1065,1095\n",
      "saved chunk 2_15 on window 1095,1125\n",
      "saved chunk 2_16 on window 1125,1155\n",
      "saved chunk 2_17 on window 1155,1185\n",
      "processing video 3\n",
      "processing window 0,406\n",
      "saved chunk 3_0 on window 0,30\n",
      "saved chunk 3_1 on window 30,60\n",
      "saved chunk 3_2 on window 60,90\n",
      "saved chunk 3_3 on window 90,120\n",
      "saved chunk 3_4 on window 120,150\n",
      "saved chunk 3_5 on window 150,180\n",
      "saved chunk 3_6 on window 180,210\n",
      "saved chunk 3_7 on window 210,240\n",
      "saved chunk 3_8 on window 240,270\n",
      "saved chunk 3_9 on window 270,300\n",
      "saved chunk 3_10 on window 300,330\n",
      "saved chunk 3_11 on window 330,360\n",
      "saved chunk 3_12 on window 360,390\n",
      "processing window 501,503\n",
      "skipping window 501,503 because it's too short\n",
      "processing window 504,513\n",
      "skipping window 504,513 because it's too short\n",
      "processing window 514,689\n",
      "saved chunk 3_13 on window 514,544\n",
      "saved chunk 3_14 on window 544,574\n",
      "saved chunk 3_15 on window 574,604\n",
      "saved chunk 3_16 on window 604,634\n",
      "saved chunk 3_17 on window 634,664\n",
      "processing window 690,691\n",
      "skipping window 690,691 because it's too short\n",
      "processing window 833,834\n",
      "skipping window 833,834 because it's too short\n",
      "processing window 835,1006\n",
      "saved chunk 3_18 on window 835,865\n",
      "saved chunk 3_19 on window 865,895\n",
      "saved chunk 3_20 on window 895,925\n",
      "saved chunk 3_21 on window 925,955\n",
      "saved chunk 3_22 on window 955,985\n",
      "processing window 1084,1085\n",
      "skipping window 1084,1085 because it's too short\n",
      "processing window 1093,1096\n",
      "skipping window 1093,1096 because it's too short\n",
      "processing window 1115,1200\n",
      "saved chunk 3_23 on window 1115,1145\n",
      "saved chunk 3_24 on window 1145,1175\n",
      "processing video 4\n",
      "processing window 0,441\n",
      "saved chunk 4_0 on window 0,30\n",
      "saved chunk 4_1 on window 30,60\n",
      "saved chunk 4_2 on window 60,90\n",
      "saved chunk 4_3 on window 90,120\n",
      "saved chunk 4_4 on window 120,150\n",
      "saved chunk 4_5 on window 150,180\n",
      "saved chunk 4_6 on window 180,210\n",
      "saved chunk 4_7 on window 210,240\n",
      "saved chunk 4_8 on window 240,270\n",
      "saved chunk 4_9 on window 270,300\n",
      "saved chunk 4_10 on window 300,330\n",
      "saved chunk 4_11 on window 330,360\n",
      "saved chunk 4_12 on window 360,390\n",
      "saved chunk 4_13 on window 390,420\n",
      "processing window 524,1192\n",
      "saved chunk 4_14 on window 524,554\n",
      "saved chunk 4_15 on window 554,584\n",
      "saved chunk 4_16 on window 584,614\n",
      "saved chunk 4_17 on window 614,644\n",
      "saved chunk 4_18 on window 644,674\n",
      "saved chunk 4_19 on window 674,704\n",
      "saved chunk 4_20 on window 704,734\n",
      "saved chunk 4_21 on window 734,764\n",
      "saved chunk 4_22 on window 764,794\n",
      "saved chunk 4_23 on window 794,824\n",
      "saved chunk 4_24 on window 824,854\n",
      "saved chunk 4_25 on window 854,884\n",
      "saved chunk 4_26 on window 884,914\n",
      "saved chunk 4_27 on window 914,944\n",
      "saved chunk 4_28 on window 944,974\n",
      "saved chunk 4_29 on window 974,1004\n",
      "saved chunk 4_30 on window 1004,1034\n",
      "saved chunk 4_31 on window 1034,1064\n",
      "saved chunk 4_32 on window 1064,1094\n",
      "saved chunk 4_33 on window 1094,1124\n",
      "saved chunk 4_34 on window 1124,1154\n",
      "saved chunk 4_35 on window 1154,1184\n"
     ]
    }
   ],
   "source": [
    "# load each mp4 video in labeled/ and split it in the max number of contiguous 30 frames chunks\n",
    "# with non-nan orientation data\n",
    "# save each chunk as a separate mp4 file as labeled_chunks/{index}_{chunk_index}.mp4\n",
    "# save the orientation data for each chunk as labeled_chunks/{index}_{chunk_index}.txt\n",
    "# where index is the index of the original video and chunk_index is the index of the chunk\n",
    "# in the original video\n",
    "if not os.path.exists('labeled_chunks'):\n",
    "    os.mkdir('labeled_chunks')\n",
    "for video_id in range(5):\n",
    "    print(f\"processing video {video_id}\")\n",
    "    vframes = videos[video_id]\n",
    "    labels = orientations[video_id]\n",
    "    windows = non_nan_windows(labels)\n",
    "    chunk_id = 0\n",
    "    for window in windows:\n",
    "        start, end = window\n",
    "        print(f\"processing window {start},{end}\")\n",
    "        if end - start < MIN_CHUNK_LENGTH:\n",
    "            print(f\"skipping window {start},{end} because it's too short\")\n",
    "            continue\n",
    "        else:\n",
    "            num_chunks = (end - start) // MIN_CHUNK_LENGTH\n",
    "            for i in range(num_chunks):\n",
    "                chunk_start = start + i * MIN_CHUNK_LENGTH\n",
    "                chunk_end = chunk_start + MIN_CHUNK_LENGTH\n",
    "                chunk = vframes[chunk_start:chunk_end]\n",
    "                chunk_orientations = labels[chunk_start:chunk_end]\n",
    "                if np.any(np.isnan(chunk_orientations)):\n",
    "                    raise ValueError(\"chunk has nan orientation data\")\n",
    "\n",
    "                torchvision.io.write_video(\n",
    "                    f\"labeled_chunks/{video_id}_{chunk_id}.mp4\", chunk, fps=20, video_codec=\"libx264\"\n",
    "                )\n",
    "                np.savetxt(f\"labeled_chunks/{video_id}_{chunk_id}.txt\", chunk_orientations)\n",
    "                print(f\"saved chunk {video_id}_{chunk_id} on window {chunk_start},{chunk_end}\")\n",
    "                chunk_id += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir = 'labeled_chunks', batch_size: int = 3) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        filenames = []\n",
    "        for filename in os.listdir(self.data_dir):\n",
    "            if len(filenames) >= 10:\n",
    "                break\n",
    "            if filename.endswith('.mp4'):\n",
    "                filenames.append(filename)\n",
    "        if len(filenames) == 0:\n",
    "            raise ValueError('no files')\n",
    "        print(filenames)\n",
    "        np.random.shuffle(filenames)\n",
    "        num_train = int(len(filenames) * 0.8)\n",
    "        num_val = int(len(filenames) * 0.1)\n",
    "        num_test = len(filenames) - num_train - num_val\n",
    "        self.train_filenames = filenames[:num_train]\n",
    "        self.val_filenames = filenames[num_train:num_train + num_val]\n",
    "        self.test_filenames = filenames[num_train + num_val:]\n",
    "        self.train_data = torch.zeros((len(self.train_filenames), MIN_CHUNK_LENGTH, HEIGHT, WIDTH, 3))\n",
    "        self.val_data = torch.zeros((len(self.val_filenames), MIN_CHUNK_LENGTH, HEIGHT, WIDTH, 3))\n",
    "        self.test_data = torch.zeros((len(self.test_filenames), MIN_CHUNK_LENGTH, HEIGHT, WIDTH, 3))\n",
    "        self.train_labels = torch.zeros((len(self.train_filenames), MIN_CHUNK_LENGTH, 2))\n",
    "        self.val_labels = torch.zeros((len(self.val_filenames), MIN_CHUNK_LENGTH, 2))\n",
    "        self.test_labels = torch.zeros((len(self.test_filenames), MIN_CHUNK_LENGTH, 2))\n",
    "        for i, filename in enumerate(self.train_filenames):\n",
    "            frames, _, _ = torchvision.io.read_video(\n",
    "                os.path.join(self.data_dir, filename), start_pts=0, end_pts=MIN_CHUNK_LENGTH, pts_unit=\"pts\"\n",
    "            )\n",
    "            self.train_data[i] = frames\n",
    "            self.train_labels[i] = torch.from_numpy(np.loadtxt(os.path.join(self.data_dir, filename.replace('.mp4', '.txt'))))\n",
    "        for i, filename in enumerate(self.val_filenames):\n",
    "            frames, _, _ = torchvision.io.read_video(\n",
    "                os.path.join(self.data_dir, filename), start_pts=0, end_pts=MIN_CHUNK_LENGTH, pts_unit=\"pts\"\n",
    "            )\n",
    "            self.val_data[i] = frames\n",
    "            self.val_labels[i] = torch.from_numpy(np.loadtxt(os.path.join(self.data_dir, filename.replace('.mp4', '.txt'))))\n",
    "        for i, filename in enumerate(self.test_filenames):\n",
    "            frames, _, _ = torchvision.io.read_video(\n",
    "                os.path.join(self.data_dir, filename), start_pts=0, end_pts=MIN_CHUNK_LENGTH, pts_unit=\"pts\"\n",
    "            )\n",
    "            self.test_data[i] = frames\n",
    "            self.test_labels[i] = torch.from_numpy(np.loadtxt(os.path.join(self.data_dir, filename.replace('.mp4', '.txt'))))\n",
    "\n",
    "\n",
    "    def train_dataloader(self) -> tdata.DataLoader:\n",
    "        return tdata.DataLoader(\n",
    "            tdata.TensorDataset(self.train_data, self.train_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> tdata.DataLoader:\n",
    "        return tdata.DataLoader(\n",
    "            tdata.TensorDataset(self.val_data, self.val_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self) -> tdata.DataLoader:\n",
    "        return tdata.DataLoader(\n",
    "            tdata.TensorDataset(self.test_data, self.test_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Original ConvLSTM cell as proposed by Shi et al.\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "    ):\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        if activation == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = torch.relu\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels + out_channels,\n",
    "            out_channels=4 * out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # Initialize weights for Hadamard Products\n",
    "        self.W_ci = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "        self.W_co = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "        self.W_cf = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "\n",
    "    def forward(self, X, H_prev, C_prev):\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n",
    "\n",
    "        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev)\n",
    "        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev)\n",
    "\n",
    "        # Current Cell output\n",
    "        C = forget_gate * C_prev + input_gate * self.activation(C_conv)\n",
    "\n",
    "        output_gate = torch.sigmoid(o_conv + self.W_co * C)\n",
    "\n",
    "        # Current Hidden State\n",
    "        H = output_gate * self.activation(C)\n",
    "\n",
    "        return H, C\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "    ):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # We will unroll this over time steps\n",
    "        self.convLSTMcell = ConvLSTMCell(\n",
    "            in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is a frame sequence (batch_size, num_channels, seq_len, height, width)\n",
    "\n",
    "        # Get the dimensions\n",
    "        batch_size, _, seq_len, height, width = X.size()\n",
    "\n",
    "        # Initialize output\n",
    "        output = torch.zeros(\n",
    "            batch_size, self.out_channels, seq_len, height, width, device=DEVICE\n",
    "        )\n",
    "\n",
    "        # Initialize Hidden State\n",
    "        H = torch.zeros(batch_size, self.out_channels, height, width, device=DEVICE)\n",
    "\n",
    "        # Initialize Cell Input\n",
    "        C = torch.zeros(batch_size, self.out_channels, height, width, device=DEVICE)\n",
    "\n",
    "        # Unroll over time steps\n",
    "        for time_step in range(seq_len):\n",
    "            H, C = self.convLSTMcell(X[:, :, time_step], H, C)\n",
    "\n",
    "            output[:, :, time_step] = H\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Seq2Seq(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels: int = 3,\n",
    "        num_kernels: int = 64,\n",
    "        kernel_size: tuple = (3, 3),\n",
    "        padding: tuple = (1, 1),\n",
    "        activation: str = \"relu\",\n",
    "        frame_size: tuple = (HEIGHT, WIDTH),\n",
    "        num_layers: int = 1,\n",
    "    ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.sequential = nn.Sequential()\n",
    "\n",
    "        # Add First layer (Different in_channels than the rest)\n",
    "        self.sequential.add_module(\n",
    "            \"convlstm1\",\n",
    "            ConvLSTM(\n",
    "                in_channels=num_channels,\n",
    "                out_channels=num_kernels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                activation=activation,\n",
    "                frame_size=frame_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.sequential.add_module(\n",
    "            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "        )\n",
    "\n",
    "        # Add rest of the layers\n",
    "        for l in range(2, num_layers + 1):\n",
    "            self.sequential.add_module(\n",
    "                f\"convlstm{l}\",\n",
    "                ConvLSTM(\n",
    "                    in_channels=num_kernels,\n",
    "                    out_channels=num_kernels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    activation=activation,\n",
    "                    frame_size=frame_size,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            self.sequential.add_module(\n",
    "                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "            )\n",
    "\n",
    "            # Add Convolutional Layer to predict output frame\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=num_kernels,\n",
    "            out_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation through all the layers\n",
    "        output = self.sequential(X)\n",
    "\n",
    "        # Return only the last output frame\n",
    "        output = self.conv(output[:, :, -1])\n",
    "\n",
    "        return nn.Sigmoid()(output)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EarlyStopping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[1;32m      3\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m      4\u001b[0m     progress_bar_refresh_rate\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[0;32m----> 5\u001b[0m     callbacks\u001b[39m=\u001b[39m[EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)],\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EarlyStopping' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=2,\n",
    ")\n",
    "model = Seq2Seq()\n",
    "data_module = DataModule(data_dir='/kaggle/input/openai-calibration-challenge/labeled_chunks')\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fee673dc4fc01db422c313512934603878a992984f49c83d6d8be337f7be7ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
